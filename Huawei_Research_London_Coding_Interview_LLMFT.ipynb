{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57a8f29-9121-4d34-ba00-f9b758636ca5",
   "metadata": {},
   "source": [
    "# Building a Custom Trainer Based on Huggingface \n",
    "\n",
    "## Main Task\n",
    "In this exercise, you need to implement a trainer based on Huggingface Trainer class. \n",
    "You need to extend the existing Trainer class of huggingface to use a different loss function.\n",
    "Below is the link to the documentation of the Trainer class: https://huggingface.co/docs/transformers/main/en/trainer\n",
    "Note that you need 4-8GB of CPU RAM.\n",
    "We do not expect to run the full training, but only to implement the necessary components for training\n",
    "\n",
    "## LLM\n",
    "For this excersice you need to use the Qwen/Qwen1.5-0.5B-Chat model.\n",
    "Note: THIS IS A CHAT MODEL. Please read carefully how to use this model in: https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat\n",
    "\n",
    "## Dataset\n",
    "\n",
    "You are given the training dataset where each example in the dataset is a dictionary that contains two keys:\n",
    "* The first key is 'text', which contains a multi-turn conversation in natural language that will be used as input to the llm. The text is in the form [list[dict]], which is a list of dictionaries. Each dictionary has two keys; the first is the role, which can be 'system', 'user', or 'assistant', the second is 'content', which is the content of the message. 'system' corresponds to the system prompt of the llm, 'user' corresponds to the text that is inputted to the llm, and 'assistant' corresponds to the response of the llm.\n",
    "```\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You a helpful assisant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Fine. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the circumference of earth?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"It is 40,075 kms.\"},]\n",
    "```\n",
    "\n",
    "* The second key is 'reward', which is a scalar number between -1 and 1, which indicates whether the specific example is good or not\n",
    "\n",
    "## Optimization Objective\n",
    "\n",
    "You need to implement the undiscounted REINFORCE algorithm. It is an extension of SFT that takes into account the reward that is assigned to the trajectory.\n",
    "The loss function is \n",
    "$$ L = - \\frac{1}{B} \\sum_{b \\in B}\\sum_{t \\in seq[b]} (reward[b] * \\log p(x[b][t] | x[b][:t])) \\textrm{ if $x[b][t]$ is one of the assistant's tokens}$$\n",
    "\n",
    "The loss is 0 otherwise.\n",
    "Practically that means that give the aforementioned chat example, the loss is not 0 only for the following pieces of text\n",
    "```\n",
    "{\"role\": \"assistant\", \"content\": \"Fine. How can I help you today?\"},\n",
    "and\n",
    "{\"role\": \"assistant\", \"content\": \"It is 40,075 kms.\"}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ff6956-eae3-4e37-8bf5-cace49a53063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=dtype,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ff93f-2feb-4941-a4af-b0c083126624",
   "metadata": {},
   "source": [
    "You are give the following data, which has to be loaded in order to be used by the huggingface's transformers Trainer. We recommend using the datasets library \n",
    "https://huggingface.co/docs/datasets/en/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3babd8b0-4494-4cdf-9c23-11b52c9b20d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': [{'role': 'system', 'content': 'You a helpful assisant'}, {'role': 'user', 'content': 'Hello, how are you?'}, {'role': 'assistant', 'content': 'Fine. How can I help you today?'}, {'role': 'user', 'content': 'What is the circumference of earth?'}, {'role': 'assistant', 'content': 'It is 40,075 kms.'}], 'reward': 1}, {'text': [{'role': 'system', 'content': 'You a helpful assisant'}, {'role': 'user', 'content': 'Hello, how are you?'}, {'role': 'assistant', 'content': 'Fine. How can I help you today?'}, {'role': 'user', 'content': 'What is the shape of earth?'}, {'role': 'assistant', 'content': 'Earth is a square'}], 'reward': -1}]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"text\": [\n",
    "            {\"role\": \"system\", \"content\": \"You a helpful assisant\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Fine. How can I help you today?\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the circumference of earth?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"It is 40,075 kms.\"},\n",
    "        ],\n",
    "        \"reward\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"text\": [\n",
    "            {\"role\": \"system\", \"content\": \"You a helpful assisant\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Fine. How can I help you today?\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the shape of earth?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Earth is a square\"},\n",
    "        ],\n",
    "        \"reward\": -1,\n",
    "    },\n",
    "]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f84a36-081b-4807-85f2-8ca2d5d3c0ac",
   "metadata": {},
   "source": [
    "Please note that we do not expect to run the full training. We will just check whether the code is generally correct. You are free to use any library such as transformers, trl, etc for your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb1b5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f14f831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    264,  10950,   1071,    285,    517,\n",
       "         151645,    198, 151644,    872,    198,   9707,     11,   1246,    525,\n",
       "            498,     30, 151645,    198, 151644,  77091,    198,  63716,     13,\n",
       "           2585,    646,    358,   1492,    498,   3351,     30, 151645,    198,\n",
       "         151644,    872,    198,   3838,    374,    279,  74926,    315,   9393,\n",
       "             30, 151645,    198, 151644,  77091,    198,   2132,    374,    220,\n",
       "             19,     15,     11,     15,     22,     20,  96677,     13, 151645,\n",
       "            198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'assistant_masks': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    data[0][\"text\"],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=False,\n",
    "    return_dict=True,\n",
    "    return_assistant_tokens_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f07af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou a helpful assisant<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\nFine. How can I help you today?<|im_end|>\\n<|im_start|>user\\nWhat is the circumference of earth?<|im_end|>\\n<|im_start|>assistant\\nIt is 40,075 kms.<|im_end|>\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(data[0][\"text\"], tokenize=False, add_generation_prompt=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25d4ed66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644, 77091, 198, 151645, 198]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|im_start|>assistant\\n<|im_end|>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644, 77091, 198, 32, 151645, 198]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|im_start|>assistant\\nA<|im_end|>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d031d7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644, 77091, 198, 32, 151645]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|im_start|>assistant\\nA<|im_end|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68c15259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    264,  10950,   1071,    285,    517,\n",
       "         151645,    198, 151644,    872,    198,   9707,     11,   1246,    525,\n",
       "            498,     30, 151645,    198, 151644,  77091,    198,  63716,     13,\n",
       "           2585,    646,    358,   1492,    498,   3351,     30, 151645,    198,\n",
       "         151644,    872,    198,   3838,    374,    279,  74926,    315,   9393,\n",
       "             30, 151645,    198, 151644,  77091,    198,   2132,    374,    220,\n",
       "             19,     15,     11,     15,     22,     20,  96677,     13, 151645,\n",
       "            198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    data[0][\"text\"],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=False,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "11ad8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assistant_start = tokenizer.encode(\"<|im_start|>assistant\\n\", return_tensors=\"pt\")[0]\n",
    "assistant_end = tokenizer.encode(\"<|im_end|>\", return_tensors=\"pt\")[0]\n",
    "\n",
    "\n",
    "def preprocess_dataset(entry):\n",
    "    out = tokenizer.apply_chat_template(\n",
    "        entry[\"text\"],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    out[\"input_ids\"] = out[\"input_ids\"].squeeze()\n",
    "    out[\"attention_mask\"] = out[\"attention_mask\"].squeeze()\n",
    "\n",
    "    out[\"reward\"] = torch.tensor(entry[\"reward\"], dtype=torch.float32)\n",
    "\n",
    "    assistant_mask = torch.zeros_like(out[\"input_ids\"], dtype=torch.bool)\n",
    "    start_indices = (\n",
    "        (out[\"input_ids\"].unfold(0, len(assistant_start), 1) == assistant_start).all(dim=1).nonzero(as_tuple=True)[0]\n",
    "    )\n",
    "    end_indices = (\n",
    "        (out[\"input_ids\"].unfold(0, len(assistant_end), 1) == assistant_end).all(dim=1).nonzero(as_tuple=True)[0]\n",
    "    )\n",
    "    # print(start_indices)\n",
    "    # print(end_indices)\n",
    "    for start_idx in start_indices:\n",
    "        end_idx = end_indices[end_indices > start_idx][0]\n",
    "        # print(start_idx, end_idx)\n",
    "        assistant_mask[start_idx + len(assistant_start) : end_idx] = True\n",
    "    out[\"assistant_mask\"] = assistant_mask\n",
    "\n",
    "    labels = out[\"input_ids\"].clone()\n",
    "    labels[~assistant_mask] = -100\n",
    "    out[\"labels\"] = labels\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b4190420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "        151645,    198, 151644,  77091,    198,     32, 151645,    198]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'reward': tensor(42.), 'assistant_mask': tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100,   32, -100, -100])}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_dataset({\"text\": [{\"role\": \"assistant\", \"content\": \"A\"}], \"reward\": 42})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c8bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f1c422e3f34ab39ef8e58d0c373ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['reward', 'input_ids', 'attention_mask', 'assistant_mask', 'labels'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset = dataset.map(preprocess_dataset, remove_columns=[\"text\", \"reward\"])\n",
    "\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\", \"assistant_mask\", \"reward\"])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a0e75152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collated Batch Example:\n",
      "input_ids: shape=torch.Size([2, 64]), dtype=torch.int64\n",
      "attention_mask: shape=torch.Size([2, 64]), dtype=torch.int64\n",
      "labels: shape=torch.Size([2, 64]), dtype=torch.int64\n",
      "assistant_mask: shape=torch.Size([2, 64]), dtype=torch.bool\n",
      "reward: shape=torch.Size([2]), dtype=torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aigoncharov/dev/huawei_RE_test/.venv/lib/python3.12/site-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "from transformers.data.data_collator import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "class ReinforceDataCollator(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        if return_tensors is None:\n",
    "            return_tensors = self.return_tensors\n",
    "\n",
    "        assistant_masks = [feature.pop(\"assistant_mask\") for feature in features]\n",
    "        rewards = [feature.pop(\"reward\") for feature in features]\n",
    "\n",
    "        batch = super().__call__(features, return_tensors=return_tensors)\n",
    "\n",
    "        max_length = batch[\"input_ids\"].shape[1]\n",
    "        padded_assistant_mask = torch.zeros((len(features), max_length), dtype=assistant_masks[0].dtype)\n",
    "        for i, mask in enumerate(assistant_masks):\n",
    "            padded_assistant_mask[i, : len(mask)] = mask\n",
    "        batch[\"assistant_mask\"] = padded_assistant_mask\n",
    "\n",
    "        batch[\"reward\"] = torch.tensor(rewards, dtype=torch.float32)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = ReinforceDataCollator(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "# Test collator with a small batch\n",
    "sample_batch = [dataset[0], dataset[1]]\n",
    "collated_batch = data_collator(sample_batch)\n",
    "print(\"\\nCollated Batch Example:\")\n",
    "for key, value in collated_batch.items():\n",
    "    print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcc9a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer import Trainer\n",
    "\n",
    "\n",
    "class ReinforceTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        rewards = inputs.pop(\"reward\")\n",
    "        assistant_mask = inputs.pop(\"assistant_mask\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Shift logits and labels for standard causal LM loss calculation\n",
    "        # loss(logits[..., :-1, :], labels[..., 1:])\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_assistant_mask = assistant_mask[..., 1:].contiguous()  # Shift mask accordingly\n",
    "\n",
    "        # Calculate log probabilities of actual tokens (labels)\n",
    "        # Use cross_entropy with reduction='none' to get per-token loss\n",
    "        # log p(token) = - cross_entropy(logits, token_id)\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        # Reshape for CrossEntropyLoss: (batch_size * seq_len, vocab_size), (batch_size * seq_len)\n",
    "        log_probs_per_token = -loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        # Reshape back to (batch_size, seq_len - 1)\n",
    "        log_probs_per_token = log_probs_per_token.view(shift_logits.size(0), shift_logits.size(1))\n",
    "\n",
    "        # Apply the assistant mask: Zero out log_probs for non-assistant tokens\n",
    "        # Only consider loss for tokens where the *label* is not -100 (i.e., not padding or masked)\n",
    "        # and where the token belongs to the assistant\n",
    "        valid_token_mask = (shift_labels != -100) & shift_assistant_mask\n",
    "        masked_log_probs = log_probs_per_token * valid_token_mask\n",
    "\n",
    "        # Weight the log probabilities by the reward\n",
    "        # rewards shape: (batch_size), need (batch_size, 1) to broadcast\n",
    "        reward_weighted_log_probs = masked_log_probs * rewards.unsqueeze(-1)\n",
    "\n",
    "        # Calculate the loss for each sequence (sum over sequence length)\n",
    "        # We sum the reward-weighted log probs according to the formula\n",
    "        # The negative sign is handled because we started with negative log probs (-cross_entropy)\n",
    "        # L = - Σ reward * log_prob => Minimize this\n",
    "        # Our current value = Σ reward * (-cross_entropy) = Σ reward * log_prob\n",
    "        # So we need to sum this up and negate it for the final loss\n",
    "        per_sequence_loss = reward_weighted_log_probs.sum(dim=-1)  # Sum across the sequence dimension\n",
    "\n",
    "        # Average the loss over the batch\n",
    "        # Ensure we only average over sequences that had at least one assistant token\n",
    "        # Although if reward is 0 or log_prob is 0, sum will be 0 anyway.\n",
    "        # A simple mean is usually sufficient here.\n",
    "        loss = -per_sequence_loss.mean()  # Negate for minimization\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2efa918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# --- Instantiate Custom Trainer ---\n",
    "trainer = ReinforceTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,  # Use the custom collator\n",
    ")\n",
    "\n",
    "# --- Run a training step (or two) to check ---\n",
    "print(\"\\nStarting dummy training step...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Dummy training step(s) completed without runtime errors.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during dummy training step: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
