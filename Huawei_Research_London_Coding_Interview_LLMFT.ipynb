{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57a8f29-9121-4d34-ba00-f9b758636ca5",
   "metadata": {},
   "source": [
    "# Building a Custom Trainer Based on Huggingface \n",
    "\n",
    "## Main Task\n",
    "In this exercise, you need to implement a trainer based on Huggingface Trainer class. \n",
    "You need to extend the existing Trainer class of huggingface to use a different loss function.\n",
    "Below is the link to the documentation of the Trainer class: https://huggingface.co/docs/transformers/main/en/trainer\n",
    "Note that you need 4-8GB of CPU RAM.\n",
    "We do not expect to run the full training, but only to implement the necessary components for training\n",
    "\n",
    "## LLM\n",
    "For this excersice you need to use the Qwen/Qwen1.5-0.5B-Chat model.\n",
    "Note: THIS IS A CHAT MODEL. Please read carefully how to use this model in: https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat\n",
    "\n",
    "## Dataset\n",
    "\n",
    "You are given the training dataset where each example in the dataset is a dictionary that contains two keys:\n",
    "* The first key is 'text', which contains a multi-turn conversation in natural language that will be used as input to the llm. The text is in the form [list[dict]], which is a list of dictionaries. Each dictionary has two keys; the first is the role, which can be 'system', 'user', or 'assistant', the second is 'content', which is the content of the message. 'system' corresponds to the system prompt of the llm, 'user' corresponds to the text that is inputted to the llm, and 'assistant' corresponds to the response of the llm.\n",
    "```\n",
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You a helpful assisant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Fine. How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the circumference of earth?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"It is 40,075 kms.\"},]\n",
    "```\n",
    "\n",
    "* The second key is 'reward', which is a scalar number between -1 and 1, which indicates whether the specific example is good or not\n",
    "\n",
    "## Optimization Objective\n",
    "\n",
    "You need to implement the undiscounted REINFORCE algorithm. It is an extension of SFT that takes into account the reward that is assigned to the trajectory.\n",
    "The loss function is \n",
    "$$ L = - \\frac{1}{B} \\sum_{b \\in B}\\sum_{t \\in seq[b]} (reward[b] * \\log p(x[b][t] | x[b][:t])) \\textrm{ if $x[b][t]$ is one of the assistant's tokens}$$\n",
    "\n",
    "The loss is 0 otherwise.\n",
    "Practically that means that give the aforementioned chat example, the loss is not 0 only for the following pieces of text\n",
    "```\n",
    "{\"role\": \"assistant\", \"content\": \"Fine. How can I help you today?\"},\n",
    "and\n",
    "{\"role\": \"assistant\", \"content\": \"It is 40,075 kms.\"}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c2ff6956-eae3-4e37-8bf5-cace49a53063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"Qwen/Qwen1.5-0.5B-Chat\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=dtype,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96ff93f-2feb-4941-a4af-b0c083126624",
   "metadata": {},
   "source": [
    "You are give the following data, which has to be loaded in order to be used by the huggingface's transformers Trainer. We recommend using the datasets library \n",
    "https://huggingface.co/docs/datasets/en/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3babd8b0-4494-4cdf-9c23-11b52c9b20d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': [{'role': 'system', 'content': 'You a helpful assisant'}, {'role': 'user', 'content': 'Hello, how are you?'}, {'role': 'assistant', 'content': 'Fine. How can I help you today?'}, {'role': 'user', 'content': 'What is the circumference of earth?'}, {'role': 'assistant', 'content': 'It is 40,075 kms.'}], 'reward': 1}, {'text': [{'role': 'system', 'content': 'You a helpful assisant'}, {'role': 'user', 'content': 'Hello, how are you?'}, {'role': 'assistant', 'content': 'Fine. How can I help you today?'}, {'role': 'user', 'content': 'What is the shape of earth?'}, {'role': 'assistant', 'content': 'Earth is a square'}], 'reward': -1}]\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"text\": [\n",
    "            {\"role\": \"system\", \"content\": \"You a helpful assisant\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Fine. How can I help you today?\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the circumference of earth?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"It is 40,075 kms.\"},\n",
    "        ],\n",
    "        \"reward\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"text\": [\n",
    "            {\"role\": \"system\", \"content\": \"You a helpful assisant\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Fine. How can I help you today?\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is the shape of earth?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Earth is a square\"},\n",
    "        ],\n",
    "        \"reward\": -1,\n",
    "    },\n",
    "]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f84a36-081b-4807-85f2-8ca2d5d3c0ac",
   "metadata": {},
   "source": [
    "Please note that we do not expect to run the full training. We will just check whether the code is generally correct. You are free to use any library such as transformers, trl, etc for your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cdb1b5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eos_token': '<|im_end|>',\n",
       " 'pad_token': '<|endoftext|>',\n",
       " 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7f14f831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    264,  10950,   1071,    285,    517,\n",
       "         151645,    198, 151644,    872,    198,   9707,     11,   1246,    525,\n",
       "            498,     30, 151645,    198, 151644,  77091,    198,  63716,     13,\n",
       "           2585,    646,    358,   1492,    498,   3351,     30, 151645,    198,\n",
       "         151644,    872,    198,   3838,    374,    279,  74926,    315,   9393,\n",
       "             30, 151645,    198, 151644,  77091,    198,   2132,    374,    220,\n",
       "             19,     15,     11,     15,     22,     20,  96677,     13, 151645,\n",
       "            198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'assistant_masks': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    data[0][\"text\"],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=False,\n",
    "    return_dict=True,\n",
    "    return_assistant_tokens_mask=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1a3f07af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou a helpful assisant<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\nFine. How can I help you today?<|im_end|>\\n<|im_start|>user\\nWhat is the circumference of earth?<|im_end|>\\n<|im_start|>assistant\\nIt is 40,075 kms.<|im_end|>\\n'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(data[0][\"text\"], tokenize=False, add_generation_prompt=False, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "25d4ed66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644, 77091, 198, 151645, 198]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|im_start|>assistant\\n<|im_end|>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ea43703b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644, 77091, 198, 32, 151645, 198]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|im_start|>assistant\\nA<|im_end|>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d031d7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644, 77091, 198, 32, 151645]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"<|im_start|>assistant\\nA<|im_end|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "68c15259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,   8948,    198,   2610,    264,  10950,   1071,    285,    517,\n",
       "         151645,    198, 151644,    872,    198,   9707,     11,   1246,    525,\n",
       "            498,     30, 151645,    198, 151644,  77091,    198,  63716,     13,\n",
       "           2585,    646,    358,   1492,    498,   3351,     30, 151645,    198,\n",
       "         151644,    872,    198,   3838,    374,    279,  74926,    315,   9393,\n",
       "             30, 151645,    198, 151644,  77091,    198,   2132,    374,    220,\n",
       "             19,     15,     11,     15,     22,     20,  96677,     13, 151645,\n",
       "            198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(\n",
    "    data[0][\"text\"],\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=False,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "11ad8d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "assistant_start = tokenizer.encode(\"<|im_start|>assistant\\n\", return_tensors=\"pt\")[0]\n",
    "assistant_end = tokenizer.encode(\"<|im_end|>\", return_tensors=\"pt\")[0]\n",
    "\n",
    "\n",
    "def tokenize_dataset_entry(entry):\n",
    "    out = tokenizer.apply_chat_template(\n",
    "        entry[\"text\"],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=False,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    out[\"input_ids\"] = out[\"input_ids\"].squeeze()\n",
    "    out[\"attention_mask\"] = out[\"attention_mask\"].squeeze()\n",
    "\n",
    "    out[\"reward\"] = torch.tensor([entry[\"reward\"]], dtype=torch.float32)\n",
    "\n",
    "    assistant_mask = torch.zeros_like(out[\"input_ids\"], dtype=torch.bool)\n",
    "    start_indices = (\n",
    "        (out[\"input_ids\"].unfold(0, len(assistant_start), 1) == assistant_start).all(dim=1).nonzero(as_tuple=True)[0]\n",
    "    )\n",
    "    end_indices = (\n",
    "        (out[\"input_ids\"].unfold(0, len(assistant_end), 1) == assistant_end).all(dim=1).nonzero(as_tuple=True)[0]\n",
    "    )\n",
    "    # print(start_indices)\n",
    "    # print(end_indices)\n",
    "    for start_idx in start_indices:\n",
    "        end_idx = end_indices[end_indices > start_idx][0]\n",
    "        # print(start_idx, end_idx)\n",
    "        assistant_mask[start_idx + len(assistant_start) : end_idx] = True\n",
    "    out[\"assistant_mask\"] = assistant_mask\n",
    "\n",
    "    labels = out[\"input_ids\"].clone()\n",
    "    labels[~assistant_mask] = -100\n",
    "    out[\"labels\"] = labels\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b4190420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "        151645,    198, 151644,  77091,    198,     32, 151645,    198]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), 'reward': tensor([42.]), 'assistant_mask': tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False]), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100,   32, -100, -100])}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_dataset_entry({\"text\": [{\"role\": \"assistant\", \"content\": \"A\"}], \"reward\": 42})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "415c8bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'reward'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a0e75152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collated Batch Example:\n",
      "input_ids: shape=torch.Size([2, 64]), dtype=torch.int64\n",
      "attention_mask: shape=torch.Size([2, 64]), dtype=torch.int64\n",
      "labels: shape=torch.Size([2, 64]), dtype=torch.int64\n",
      "assistant_mask: shape=torch.Size([2, 64]), dtype=torch.bool\n",
      "reward: shape=torch.Size([2, 1]), dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "from transformers.data.data_collator import DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "class ReinforceDataCollator(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        if return_tensors is None:\n",
    "            return_tensors = self.return_tensors\n",
    "\n",
    "        processed_entries = [tokenize_dataset_entry(feature) for feature in features]\n",
    "\n",
    "        assistant_masks = [entry.pop(\"assistant_mask\") for entry in processed_entries]\n",
    "        rewards = [entry.pop(\"reward\") for entry in processed_entries]\n",
    "\n",
    "        batch = super().__call__(processed_entries, return_tensors=return_tensors)\n",
    "\n",
    "        max_length = batch[\"input_ids\"].shape[1]\n",
    "        padded_assistant_mask = torch.zeros((len(features), max_length), dtype=assistant_masks[0].dtype)\n",
    "        for i, mask in enumerate(assistant_masks):\n",
    "            padded_assistant_mask[i, : len(mask)] = mask\n",
    "        batch[\"assistant_mask\"] = padded_assistant_mask\n",
    "\n",
    "        batch[\"reward\"] = torch.stack(rewards, dim=0)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = ReinforceDataCollator(tokenizer=tokenizer, padding=True)\n",
    "\n",
    "# Test collator with a small batch\n",
    "sample_batch = [dataset[0], dataset[1]]\n",
    "collated_batch = data_collator(sample_batch)\n",
    "print(\"\\nCollated Batch Example:\")\n",
    "for key, value in collated_batch.items():\n",
    "    print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bcc9a1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer import Trainer\n",
    "\n",
    "\n",
    "class ReinforceTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        rewards = inputs.pop(\"reward\")\n",
    "        assistant_mask = inputs.pop(\"assistant_mask\")\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Shift to match predictions\n",
    "        logits = logits[:, :-1, :].contiguous()\n",
    "        labels = labels[:, 1:].contiguous()\n",
    "        assistant_mask = assistant_mask[:, 1:].contiguous()\n",
    "\n",
    "        batch_size, seq_len, vocab_size = logits.shape\n",
    "\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        log_probs_per_token = loss_fct(logits.view(batch_size * seq_len, vocab_size), labels.view(batch_size * seq_len))\n",
    "        log_probs_per_token = log_probs_per_token.view(batch_size, seq_len)\n",
    "\n",
    "        masked_log_probs = log_probs_per_token * assistant_mask\n",
    "        # print(masked_log_probs)\n",
    "        reward_weighted_log_probs = masked_log_probs * rewards\n",
    "        # print(reward_weighted_log_probs)\n",
    "\n",
    "        per_sequence_loss = reward_weighted_log_probs.sum(dim=-1)\n",
    "\n",
    "        loss = per_sequence_loss.mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f2efa918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>12.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-1.916600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-23.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-32.624500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-40.076600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=-17.12502956390381, metrics={'train_runtime': 4.9174, 'train_samples_per_second': 2.034, 'train_steps_per_second': 1.017, 'total_flos': 1184276152320.0, 'train_loss': -17.12502956390381, 'epoch': 5.0})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=1,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = ReinforceTrainer(model=model, args=training_args, train_dataset=dataset, data_collator=data_collator)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
